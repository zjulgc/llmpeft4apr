
'====== llmpeft4apr/results/CodeLlama_7b_hf_no_no_on_humaneval_validation_20240407_103144.json results:===='
Model: CodeLlama-7b-hf
PEFT Method: no  
train_dataset: no  
validation benchmark : humaneval 
validation result: 
pass@0:plausible pathces - 0, total  problems - 163, correctness pecent - 0.0
pass@1:plausible pathces - 46, total  problems - 163, correctness pecent - 0.2822085889570552
pass@2:plausible pathces - 56, total  problems - 163, correctness pecent - 0.34355828220858897
pass@3:plausible pathces - 61, total  problems - 163, correctness pecent - 0.37423312883435583
pass@4:plausible pathces - 67, total  problems - 163, correctness pecent - 0.4110429447852761
pass@5:plausible pathces - 67, total  problems - 163, correctness pecent - 0.4110429447852761
pass@6:plausible pathces - 69, total  problems - 163, correctness pecent - 0.4233128834355828
pass@7:plausible pathces - 71, total  problems - 163, correctness pecent - 0.43558282208588955
pass@8:plausible pathces - 72, total  problems - 163, correctness pecent - 0.44171779141104295
pass@9:plausible pathces - 72, total  problems - 163, correctness pecent - 0.44171779141104295
pass@10:plausible pathces - 73, total  problems - 163, correctness pecent - 0.44785276073619634
